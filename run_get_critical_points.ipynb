{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok avec `PoinNet0_env`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applique le script get_cirtical_points.py à tous les fichiers .off d'un répertoire (`folder_path+dataset+\"/\"`) et enregistre les outputs dans le dossier `output_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "Dataset_path = '/home/pelissier/These-ATER/Papier_international3/Dataset/'\n",
    "Critical_pts_folder_path = '/home/pelissier/These-ATER/Papier_international3/PointNet/my_PointNet/Critical_pts/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_off_files_recursive(folder_path, extension='*.off'):\n",
    "    # Get all .off files from the folder and its subfolders\n",
    "    off_files = glob.glob(os.path.join(folder_path, \"**\", extension), recursive=True)\n",
    "    \n",
    "    # Filter out files that contain 'SIMPL' in their name\n",
    "    off_files = [file for file in off_files if 'SMPL' not in os.path.basename(file)]\n",
    "    \n",
    "    return off_files\n",
    "\n",
    "def count_files_in_folder_recursive(folder_path):\n",
    "    file_count = 0\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "    return file_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ModelNet40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "dataset = \"ModelNet40\"\n",
    "# Tous les path des fichiers .off de ModelNet40\n",
    "#off_files = get_off_files_recursive(Dataset_path+dataset+\"/\"); print(f\"Il y a : {len(off_files)} fichiers .off dans le dataset {dataset}\")\n",
    "\n",
    "# Ceux avec des pbl : paths du fichier 'failed_files.txt'\n",
    "failed_files = []\n",
    "# Off file paths with the error\n",
    "with open(Critical_pts_folder_path+'failed_files.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        failed_files.append(line[:-1])  # Print each line, stripping leading/trailing whitespace\n",
    "off_files = failed_files; print(f\"Il y a : {len(off_files)} fichiers .off avec des pbl dans le dataset {dataset}\")\n",
    "\n",
    "# Creation du dossier de sortie : Critical_pts/ModelNet40\n",
    "# Check if folder exists, and if not, create it\n",
    "modelnet40_path = Critical_pts_folder_path+dataset+\"/\"\n",
    "if not Path(modelnet40_path).exists(): Path(modelnet40_path).mkdir(parents=True, exist_ok=True)\n",
    "else : print(\"Le dossier \"+dataset+\" existe déjà !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les catégories de ModelNet40 \n",
    "categories = os.listdir(Dataset_path+dataset+\"/\"); print(len(categories))\n",
    "\n",
    "# Create one folder for each category in the output folder : Critical_pts/ModelNet40/category\n",
    "for category in categories:\n",
    "    category_folder = os.path.join(modelnet40_path, category)\n",
    "    if not Path(category_folder).exists():Path(category_folder).mkdir(parents=True, exist_ok=True)\n",
    "    #else: print(f\"Le dossier {category} existe déjà !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Path to the toto.py script\n",
    "kind_of_outputs = '{\"critical and non-critical points\": true, \"only critical points\": true, \"objet\": true}'\n",
    "affichage = \"True\"  # or \"False\", as required\n",
    "\n",
    "# Path to save failed file names\n",
    "failed_files_log = Critical_pts_folder_path+'failed_files_v2.txt'\n",
    "\n",
    "\n",
    "def run_script(off_file):\n",
    "    category = \"_\".join(os.path.basename(off_file).split('_')[:-1])\n",
    "    category_folder = os.path.join(Critical_pts_folder_path, dataset, category)+'/'\n",
    "    \n",
    "    cmd = [\n",
    "        \"python\", \n",
    "        \"/home/pelissier/These-ATER/Papier_international3/PointNet/my_PointNet/get_critical_points.py\", \n",
    "        off_file, \n",
    "        category_folder, \n",
    "        \"--kind_of_outputs\", kind_of_outputs, \n",
    "        \"--affichage\", affichage\n",
    "    ]\n",
    "    #try:\n",
    "    # Capture output and error, prevent displaying errors\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    # Check if the process succeeded (returncode 0 means success)\n",
    "    if result.returncode != 0:\n",
    "        # If there is an error, return the file name to log it\n",
    "        return off_file\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     # Log the file if any exception occurs\n",
    "    #     return off_file\n",
    "    \n",
    "    return None  # No errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import concurrent.futures\n",
    "\n",
    "# failed_files = []\n",
    "\n",
    "# # Using ThreadPoolExecutor for parallel execution\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:  # Adjust max_workers as needed\n",
    "#     # Submit tasks to the executor\n",
    "#     futures = {executor.submit(run_script, off_file): off_file for off_file in off_files}\n",
    "    \n",
    "#     for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "#         if i%250==0:\n",
    "#             print(f\"Objectif : {(len(off_files)-len(failed_files))*5}, Créé : {count_files_in_folder_recursive(modelnet40_path)}, Pbl : {len(failed_files)}\")\n",
    "#         try:\n",
    "#             result = future.result()\n",
    "#             if result is not None:\n",
    "#                 failed_files.append(result)\n",
    "#         except Exception as exc:\n",
    "#             pass#print(exc)\n",
    "\n",
    "# # Save the failed file names to a log file\n",
    "# if failed_files:\n",
    "#     with open(failed_files_log, 'w') as f:\n",
    "#         for file in failed_files:\n",
    "#             f.write(f\"{file}\\n\")\n",
    "# else : \n",
    "#     with open(failed_files_log, 'w') as f : f.write(\"Tout est ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(9843+2468)*5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction failed_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première ligne de ces fichiers étaient : 'OFFXXX XXX XXX\\n', alors que cela devrait être sur deux lignes : 'OFF\\nXXX XXX XXX\\n'. Le code suivant corrige les fichiers erronées en changeant unqiement la première ligne.\n",
    "\n",
    "Date : 20 oct. 2024, normalement les fichiers sont tous OK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_files = []\n",
    "\n",
    "# Off file paths with the error\n",
    "with open(Critical_pts_folder_path+'failed_files.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        failed_files.append(line[:-1])  # Print each line, stripping leading/trailing whitespace\n",
    "\n",
    "# Read the first line of each failed file\n",
    "for file_path in failed_files:\n",
    "    with open(file_path, 'r') as file:\n",
    "        first_line = file.readline().strip()  # Read and strip the first line\n",
    "        #print(first_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_off_file(off_file_path, new_first_line):\n",
    "    try:\n",
    "        # Read the existing content of the OFF file\n",
    "        with open(off_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Modify the first line (or any specific lines)\n",
    "        lines[0] = lines[0].replace('OFF', 'OFF\\n', 1)\n",
    "\n",
    "        # Write the modified content back to the same file (or a new file)\n",
    "        with open(off_file_path, 'w') as file:\n",
    "            file.writelines(lines)\n",
    "    \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {off_file_path}: {e}\")\n",
    "\n",
    "# Define the new first line\n",
    "new_first_line = \"NEW FIRST LINE CONTENT\"\n",
    "\n",
    "# Process each OFF file in the list\n",
    "#for off_file in tqdm(failed_files):\n",
    "    #modify_off_file(off_file, new_first_line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import subprocess\n",
    "\n",
    "# Define the output path and other arguments\n",
    "output_path = \"/home/pelissier/These-ATER/Papier_international3/PointNet/my_PointNet/Critical_pts/ModelNet40/\"\n",
    "kind_of_outputs = '{\"critical and non-critical points\": true, \"only critical points\": true, \"objet\": true}'\n",
    "affichage = \"True\"  # or \"False\", as required\n",
    "\n",
    "# Function to run the script\n",
    "def run_script(input_path):\n",
    "    command = [\n",
    "        \"python\", \n",
    "        \"/home/pelissier/These-ATER/Papier_international3/PointNet/my_PointNet/get_critical_points.py\", \n",
    "        input_path, \n",
    "        output_path, \n",
    "        \"--kind_of_outputs\", kind_of_outputs, \n",
    "        \"--affichage\", affichage\n",
    "    ]\n",
    "\n",
    "    # Use subprocess to execute the command\n",
    "    subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "        \n",
    "\n",
    "# # Use ProcessPoolExecutor with tqdm for progress tracking\n",
    "# with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "#     # Create a tqdm progress bar\n",
    "#     with tqdm(total=len(off_files), desc=\"Processing .off files\") as pbar:\n",
    "#         # Submit tasks to the executor\n",
    "#         futures = {executor.submit(run_script, file): file for file in off_files}\n",
    "        \n",
    "#         # Update the progress bar as each task is completed\n",
    "#         for future in concurrent.futures.as_completed(futures):\n",
    "#             pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
